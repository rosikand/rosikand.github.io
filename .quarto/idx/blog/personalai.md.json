{"title":"The Next Frontiers: Personalization of AI models","markdown":{"yaml":{"date":"2022-11-05","author":"Rohan Sikand","format":{"html":{"html-math-method":"katex","css":"../styles/post.css"}}},"headingText":"The Next Frontiers: Personalization of AI models","containsRefs":false,"markdown":"\n\n\n<!-- ## <a href=\"../index.md\" class=\"home-page-nav-link\">ðŸ¦œ Rohan Sikand</a> -->\n\n<p class=\"arrow-back-header\">\n&larr; back to <a href=\"../index.html\">home</a>, <a href=\"../blog.html\">blog</a>\n</p>\n\n\n\n\nThis post is part of my &quot;The Next Frontiers&quot; series where I delve deeper into a technology that I think\nwill be pervasive across society in 5 to 15 years.\n\nThis post focuses on the personalization of AI models.\nRight now, we have these uniform mega-models that are\nhosted in the cloud, running in application layer\nproducts. For each application, the weights stay the\nsame, meaning the model itself is the same for\napplication A and application B. Perhaps this means\ngenerality, but it can also be a limitation. Instead of\nhaving a 175B-parameter model that is deployed on\napplication A and application B, what if we have\nseparate LoRa-adapted, fine-tuned models for\napplication's A and B respectively.\n\n\n- **`Computational gains`**: due to Moore's law, training ML models will become more efficient and less expensive per parameter. This enables larger models to be trained at a cheaper cost. Perhaps the real benefit of the increasing\ncomputational and hardware efficiencies is realized in the inference realm. It is not far fetched that we can have $175$B parameter models running on device in the near future.\n\n- **`Research and algorithmic gains`**: The advent of scaling laws, the scaling hypothesis, and the bitter lesson all point to the fact that methods that leverage computation\nefficiently will perform better in the long run.\nThis implies that the larger the model, the\nbetter it will perform. However, recent\nalgorithmic advances add another tangent to\nthink about. OpenAI's strawberry algorithm\nscales inference-time compute to be able to\nreason better. Notably, this advancement is\ndetached from an increasing in model size and\ntraining compute: it was all done on the\ninference side. The conseqouence of this is that\nsmaller models, embedded with these newfound\nreasoning capabilities due to inference-time\ncompute, might result in the adoption of smaller\nmodels across the application layer. More\nspecificially, reasoning increases might lead to\napplications training their own 1B-parameter\nmodels instead of relying on the ChatGPT\n175B+-model api. Another example of algorithmic\ndevelopment is that of State Space Models\n(SSM's). The premise behind these models is that\nthey scale linearly wrt the input size instead\nof quadratically like transformers do. Cartesia,\nthe company developing these models for\nreal-world use, thinks that this development\nwill enable ubiqitious intelligence at scale, on\nany device.\n\n- **`Middle-layer developments`**: \ninclude techniques such as RAG. Thinking of\nlayers, one can imagine the base layer as the\nOpenAI API model, the middle layer as something\nlike RAG, and the application (final) layer as\nthe product which is offered to the user. These\nmiddle layers essentially try to make the models\nmore tailored for application use. For some\nreasons that I cannot quite pin down yet, I am\nslightly bearish on these seeing any long-term\nadoption. I think this is due to the previous\ntwo points: if we ride the waves of (1)\ncomputational advances and (2) reasoning and\nalgorithmic advances, middle layer technologies\nlose their moat and become unnessary: what is\nthe point of RAG if we can have a small model\ntraining on the user's documents themselves? I\ndo however think there exists a $n$-to-$m$ year\nlatency where $n$ is upper bounded by $~2$ and\n$m$ is upper bounded by $~10$ in which middle\nlayer technologies like RAG stay relevant\ntowards the goal of personalization of AI\nmodels.\n\n- **`Some remaining questions`**:\n    - How do we iterively update models on the fly in presence of new data? If we deploy a small model on-device tailored for a specific user, what do we do after they added 100 more documents to their device? Do we warm start the pre-training process? Fine-tune with LoRa? If so, how would one train these models on the fly, on-device, untethered from the cloud?\n    - Perhaps we avoid all of this by &quot;in-context learning&quot; and avoid gradient updates all together.\n\n\n- **`Conlusion`**: All of the above\n                    questions and comments remain pain points for\n                    getting AI models to be more personalized.\n                    Though, the resulting increase in algorithmic\n                    and computational advances might lead us to a\n                    world where we all have a small model living in\n                    our device, trained on our own data. One that\n                    understands us a bit better. A more personalized\n                    AI model. A second brain of some sorts.","srcMarkdownNoYaml":"\n\n\n<!-- ## <a href=\"../index.md\" class=\"home-page-nav-link\">ðŸ¦œ Rohan Sikand</a> -->\n\n<p class=\"arrow-back-header\">\n&larr; back to <a href=\"../index.html\">home</a>, <a href=\"../blog.html\">blog</a>\n</p>\n\n\n\n## The Next Frontiers: Personalization of AI models\n\nThis post is part of my &quot;The Next Frontiers&quot; series where I delve deeper into a technology that I think\nwill be pervasive across society in 5 to 15 years.\n\nThis post focuses on the personalization of AI models.\nRight now, we have these uniform mega-models that are\nhosted in the cloud, running in application layer\nproducts. For each application, the weights stay the\nsame, meaning the model itself is the same for\napplication A and application B. Perhaps this means\ngenerality, but it can also be a limitation. Instead of\nhaving a 175B-parameter model that is deployed on\napplication A and application B, what if we have\nseparate LoRa-adapted, fine-tuned models for\napplication's A and B respectively.\n\n\n- **`Computational gains`**: due to Moore's law, training ML models will become more efficient and less expensive per parameter. This enables larger models to be trained at a cheaper cost. Perhaps the real benefit of the increasing\ncomputational and hardware efficiencies is realized in the inference realm. It is not far fetched that we can have $175$B parameter models running on device in the near future.\n\n- **`Research and algorithmic gains`**: The advent of scaling laws, the scaling hypothesis, and the bitter lesson all point to the fact that methods that leverage computation\nefficiently will perform better in the long run.\nThis implies that the larger the model, the\nbetter it will perform. However, recent\nalgorithmic advances add another tangent to\nthink about. OpenAI's strawberry algorithm\nscales inference-time compute to be able to\nreason better. Notably, this advancement is\ndetached from an increasing in model size and\ntraining compute: it was all done on the\ninference side. The conseqouence of this is that\nsmaller models, embedded with these newfound\nreasoning capabilities due to inference-time\ncompute, might result in the adoption of smaller\nmodels across the application layer. More\nspecificially, reasoning increases might lead to\napplications training their own 1B-parameter\nmodels instead of relying on the ChatGPT\n175B+-model api. Another example of algorithmic\ndevelopment is that of State Space Models\n(SSM's). The premise behind these models is that\nthey scale linearly wrt the input size instead\nof quadratically like transformers do. Cartesia,\nthe company developing these models for\nreal-world use, thinks that this development\nwill enable ubiqitious intelligence at scale, on\nany device.\n\n- **`Middle-layer developments`**: \ninclude techniques such as RAG. Thinking of\nlayers, one can imagine the base layer as the\nOpenAI API model, the middle layer as something\nlike RAG, and the application (final) layer as\nthe product which is offered to the user. These\nmiddle layers essentially try to make the models\nmore tailored for application use. For some\nreasons that I cannot quite pin down yet, I am\nslightly bearish on these seeing any long-term\nadoption. I think this is due to the previous\ntwo points: if we ride the waves of (1)\ncomputational advances and (2) reasoning and\nalgorithmic advances, middle layer technologies\nlose their moat and become unnessary: what is\nthe point of RAG if we can have a small model\ntraining on the user's documents themselves? I\ndo however think there exists a $n$-to-$m$ year\nlatency where $n$ is upper bounded by $~2$ and\n$m$ is upper bounded by $~10$ in which middle\nlayer technologies like RAG stay relevant\ntowards the goal of personalization of AI\nmodels.\n\n- **`Some remaining questions`**:\n    - How do we iterively update models on the fly in presence of new data? If we deploy a small model on-device tailored for a specific user, what do we do after they added 100 more documents to their device? Do we warm start the pre-training process? Fine-tune with LoRa? If so, how would one train these models on the fly, on-device, untethered from the cloud?\n    - Perhaps we avoid all of this by &quot;in-context learning&quot; and avoid gradient updates all together.\n\n\n- **`Conlusion`**: All of the above\n                    questions and comments remain pain points for\n                    getting AI models to be more personalized.\n                    Though, the resulting increase in algorithmic\n                    and computational advances might lead us to a\n                    world where we all have a small model living in\n                    our device, trained on our own data. One that\n                    understands us a bit better. A more personalized\n                    AI model. A second brain of some sorts."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","html-math-method":"katex","css":["../styles/post.css"],"output-file":"personalai.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","date":"2022-11-05","author":"Rohan Sikand"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}